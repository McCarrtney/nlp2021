{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373c19af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre source download\n",
    "\n",
    "# 36 boxes fixed feature\n",
    "# train and val\n",
    "!wget https://storage.googleapis.com/up-down-attention/trainval_36.zip\n",
    "# test\n",
    "!wget https://storage.googleapis.com/up-down-attention/test2014_36.zip\n",
    "\n",
    "!unzip \"*.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e7a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "import json\n",
    "import h5py\n",
    "import sys\n",
    "import csv\n",
    "import base64\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import os\n",
    "import mindspore.dataset as ds\n",
    "from mindspore.mindrecord import FileWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7dee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config \n",
    "\n",
    "config = edict({\n",
    "    \"output_size\": 36,  # max number of object proposals per image\n",
    "    \"output_features\": 2048,  # number of features in each object proposal\n",
    "    \"preprocessed_train_path\": 'image-train.h5',  # path where preprocessed features from the train split are saved to and loaded from\n",
    "    \"preprocessed_val_path\": 'image-val.h5',  # path where preprocessed features from the val split are saved to and loaded from\n",
    "    \"preprocessed_test_path\": 'image-test.h5',  # path where preprocessed features from the test split are saved to and loaded from\n",
    "    \"vocabulary_path\": 'vocab/vocab.json',\n",
    "    \"train_num\": 44375,\n",
    "    \"val_num\": 21435,\n",
    "    \"test_num\": 21435,\n",
    "    \"origin_train_num\": 82783,\n",
    "    \"origin_val_num\": 40504,\n",
    "    \"origin_test_num\": 40775,\n",
    "    \"max_answers\": 6000,\n",
    "    \"max_q_length\": 666,\n",
    "    # training\n",
    "    \"epoch_size\": 15,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6660207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for subset adapt\n",
    "\n",
    "def get_needed_imageid(dataset=\"test\"):\n",
    "    path = './questions/' + dataset + '.json'\n",
    "    print(path)\n",
    "    imageid_set = set()\n",
    "    with open(path, 'r') as f:\n",
    "        t = json.load(f)\n",
    "        print(len(t['questions']))\n",
    "        for question in t['questions']:\n",
    "            imageid_set.add(question['image_id'])\n",
    "    return list(imageid_set)\n",
    "\n",
    "ss = get_needed_imageid('test')\n",
    "print(len(ss), ss[:5])\n",
    "ss = get_needed_imageid('val')\n",
    "print(len(ss), ss[:5])\n",
    "ss = get_needed_imageid('train')\n",
    "print(len(ss), ss[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process image\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "def process_image_feature(dataset=\"test\"):\n",
    "    h5FilePath = config[\"preprocessed_%s_path\" % (dataset)]\n",
    "    print(\"h5 file path:\", h5FilePath)\n",
    "    \n",
    "    num = config[('%s_num' % (dataset))] # number of images in train or in val or in test\n",
    "    print('item num:', num)\n",
    "    \n",
    "    features_shape = (\n",
    "        num,\n",
    "        config.output_features,\n",
    "        config.output_size,\n",
    "    )\n",
    "    boxes_shape = (\n",
    "        num,\n",
    "        4, # top, bottom, left, right\n",
    "        config.output_size,\n",
    "    )\n",
    "    with h5py.File(h5FilePath, 'w', libver='latest') as fd:\n",
    "        features = fd.create_dataset('features', shape=features_shape, dtype='float32')\n",
    "        boxes = fd.create_dataset('boxes', shape=boxes_shape, dtype='float32')\n",
    "        coco_ids = fd.create_dataset('ids', shape=(num,), dtype='int32')\n",
    "        widths = fd.create_dataset('widths', shape=(num,), dtype='int32')\n",
    "        heights = fd.create_dataset('heights', shape=(num,), dtype='int32')\n",
    "        \n",
    "        FIELDNAMES = ['image_id', 'image_w','image_h','num_boxes', 'boxes', 'features']\n",
    "        needed_imageids = get_needed_imageid(dataset)\n",
    "        i = 0\n",
    "        \n",
    "        for Tdataset in ['train', 'eval', 'test']:\n",
    "            tsvFilePath = \"%s2014_resnet101_faster_rcnn_genome_36.tsv\" % (Tdataset)\n",
    "            print(\"tsv file path:\", tsvFilePath)\n",
    "            with open(tsvFilePath, \"r\") as tsvF:\n",
    "                reader = csv.DictReader(tsvF, delimiter='\\t', fieldnames=FIELDNAMES)\n",
    "                origin_num = config[('origin_%s_num' % (Tdataset))]\n",
    "                for _, item in enumerate(tqdm(reader, total=origin_num)):\n",
    "                    cur_id = int(item['image_id'])\n",
    "                    if cur_id not in needed_imageids:\n",
    "                        continue\n",
    "\n",
    "                    coco_ids[i] = int(item['image_id'])\n",
    "                    widths[i] = int(item['image_w'])\n",
    "                    heights[i] = int(item['image_h'])\n",
    "\n",
    "                    buf = base64.decodestring(item['features'].encode('utf8'))\n",
    "                    array = np.frombuffer(buf, dtype='float32')\n",
    "                    array = array.reshape((-1, config.output_features)).transpose() # 36*2048 -> T -> 2048*36\n",
    "                    features[i, :, :array.shape[1]] = array\n",
    "\n",
    "                    buf = base64.decodestring(item['boxes'].encode('utf8'))\n",
    "                    array = np.frombuffer(buf, dtype='float32')\n",
    "                    array = array.reshape((-1, 4)).transpose() # 36*4 -> T -> 4*36\n",
    "                    boxes[i, :, :array.shape[1]] = array\n",
    "\n",
    "                    i = i+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b445e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process vocab\n",
    "\n",
    "_special_chars = re.compile('[^a-z0-9 ]*')\n",
    "_period_strip = re.compile(r'(?!<=\\d)(\\.)(?!\\d)')\n",
    "_comma_strip = re.compile(r'(\\d)(,)(\\d)')\n",
    "_punctuation_chars = re.escape(r';/[]\"{}()=+\\_-><@`,?!')\n",
    "_punctuation = re.compile(r'([{}])'.format(re.escape(_punctuation_chars)))\n",
    "_punctuation_with_a_space = re.compile(r'(?<= )([{0}])|([{0}])(?= )'.format(_punctuation_chars))\n",
    "\n",
    "def prepare_question(q):\n",
    "    q = q.lower()[:-1]\n",
    "    q = _special_chars.sub('', q)\n",
    "    return q.split(' ')\n",
    "\n",
    "def prepare_questions(questions_json):\n",
    "    # Tokenize and normalize questions\n",
    "    questions = [q['question'] for q in questions_json['questions']]\n",
    "    for question in questions:\n",
    "        yield prepare_question(question)\n",
    "\n",
    "def process_punctuation(s):\n",
    "    if _punctuation.search(s) is None:\n",
    "        return s\n",
    "    s = _punctuation_with_a_space.sub('', s)\n",
    "    if re.search(_comma_strip, s) is not None:\n",
    "        s = s.replace(',', '')\n",
    "    s = _punctuation.sub(' ', s)\n",
    "    s = _period_strip.sub('', s)\n",
    "    return s.strip()\n",
    "    \n",
    "def prepare_answer(answer_list):\n",
    "    return list(map(process_punctuation, answer_list))\n",
    "    \n",
    "def prepare_answers(answers_json):\n",
    "    # Normalize answers\n",
    "    answers = [[a['answer'] for a in ans_dict['answers']] for ans_dict in answers_json['annotations']]\n",
    "\n",
    "    for answer_list in answers:\n",
    "        yield prepare_answer(answer_list)\n",
    "        \n",
    "def extract_vocab(iterable, top_k=None, start=0):\n",
    "    # Turns an iterable of list of tokens into a vocabulary.\n",
    "    all_tokens = itertools.chain.from_iterable(iterable)\n",
    "    counter = Counter(all_tokens)\n",
    "    if top_k:\n",
    "        most_common = counter.most_common(top_k)\n",
    "        most_common = (t for t, c in most_common)\n",
    "    else:\n",
    "        most_common = counter.keys()\n",
    "    # descending in count, then lexicographical order\n",
    "    tokens = sorted(most_common, key=lambda x: (counter[x], x), reverse=True)\n",
    "    vocab = {t: i for i, t in enumerate(tokens, start=start)}\n",
    "    return vocab\n",
    "        \n",
    "def process_questions():\n",
    "    questions = []\n",
    "    for dataset in ['train', 'val', 'test']:\n",
    "        path = './questions/' + dataset + '.json'\n",
    "        print(path)\n",
    "        with open(path, 'r') as f:\n",
    "            t = json.load(f)\n",
    "            questions.extend(list(prepare_questions(t)))\n",
    "\n",
    "    question_vocab = extract_vocab(questions, start=1)\n",
    "    data_max_length = max(map(len, questions))\n",
    "    return question_vocab, data_max_length\n",
    "\n",
    "def process_answers():\n",
    "    answers = []\n",
    "    for dataset in ['train', 'val', 'test']:\n",
    "        path = './annotations/' + dataset + '.json'\n",
    "        print(path)\n",
    "        with open(path, 'r') as f:\n",
    "            t = json.load(f)\n",
    "            answers.extend(list(prepare_answers(t)))\n",
    "            \n",
    "    answer_vocab = extract_vocab(answers, top_k=config.max_answers)\n",
    "    return answer_vocab\n",
    "        \n",
    "        \n",
    "def process_vocab():\n",
    "    save_path=config.vocabulary_path\n",
    "    question_vocab, max_len = process_questions()\n",
    "    answer_vocab = process_answers()\n",
    "    vocabs = {\n",
    "        'question': question_vocab,\n",
    "        'answer': answer_vocab,\n",
    "        'q_max_len': max_len,\n",
    "    }\n",
    "    with open(save_path, 'w') as fd:\n",
    "        json.dump(vocabs, fd)\n",
    "    \n",
    "    return vocabs\n",
    "\n",
    "process_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a13e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_question(q, vocab, max_question_length):\n",
    "    num_tokens = len(vocab)\n",
    "    vec = np.zeros(max_question_length, dtype=int)\n",
    "    vec.fill(num_tokens)\n",
    "    q = prepare_question(q)\n",
    "    for i, token in enumerate(q):\n",
    "        if i >= max_question_length:\n",
    "            break\n",
    "        index = vocab.get(token, num_tokens - 1)\n",
    "        vec[i] = index\n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768decc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_answer(answer_list, vocab):\n",
    "    num_tokens = len(vocab)\n",
    "    answer_list = [t['answer'] for t in answer_list]\n",
    "    answers = prepare_answer(answer_list)\n",
    "    a_vecs = [vocab.get(t, num_tokens-1) for t in answers]\n",
    "    a_counter = Counter(a_vecs)\n",
    "    a_label = a_counter.most_common(1)[0][0]\n",
    "    return a_vecs,a_counter,a_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4038ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab():\n",
    "    vocab_file_path = config.vocabulary_path\n",
    "    if not(os.path.isfile(vocab_file_path)):\n",
    "        process_vocab()\n",
    "    \n",
    "    vocab = None\n",
    "    with open(vocab_file_path, 'r') as f:\n",
    "        vocab = json.load(f)\n",
    "        \n",
    "    if vocab==None:\n",
    "        raise Exception(\"error in loading vocab file\")\n",
    "    return vocab\n",
    "\n",
    "vocab = load_vocab()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d00acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide generator callable\n",
    "# refers to https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#Dataset\n",
    "# mindspore docs is quite weird\n",
    "\n",
    "class Generator():\n",
    "    def __init__(self, input_list):\n",
    "        self.input_list=input_list\n",
    "    def __getitem__(self, index): # modify according to the network struct\n",
    "        return (\n",
    "            np.array(self.input_list[index]['question']),\n",
    "            np.array(self.input_list[index]['answer_counter'])\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8105b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train, val and test record\n",
    "\n",
    "def gen_mindspore_dataset(dataset=\"test\", batch_size=64, save_mindrecord=False):\n",
    "    q_file_path = \"questions/%s.json\" % (dataset)\n",
    "    a_file_path = \"annotations/%s.json\" % (dataset)\n",
    "    \n",
    "    q_dict = dict()\n",
    "    a_dict = dict()\n",
    "    \n",
    "    # load questions and answers\n",
    "    \n",
    "    with open(q_file_path, 'r') as f:\n",
    "        t = json.load(f)\n",
    "        t = t['questions']\n",
    "        for item in t:\n",
    "            q_dict[item['question_id']] = item\n",
    "    \n",
    "    with open(a_file_path, 'r') as f:\n",
    "        t = json.load(f)\n",
    "        t = t['annotations']\n",
    "        for item in t:\n",
    "            a_dict[item['question_id']] = item\n",
    "            \n",
    "    # load questions and answers done\n",
    "    \n",
    "    # load vocab\n",
    "    \n",
    "    vocab = load_vocab()\n",
    "    \n",
    "    final_sources = []\n",
    "    for qid, item in q_dict.items():\n",
    "        source = dict()\n",
    "        source['question_id'] = qid\n",
    "        source['image_id'] = item['image_id']\n",
    "        \n",
    "        q_vec = encode_question(\n",
    "            item['question'],\n",
    "            vocab['question'],\n",
    "            min(config.max_q_length, vocab['q_max_len'])\n",
    "        )\n",
    "        source['question'] = q_vec\n",
    "        \n",
    "        # search relevant answer\n",
    "        a_item = a_dict.get(qid, -1)\n",
    "        if a_item == -1:\n",
    "            print('cannot find qid %s in answer dict' % qid)\n",
    "            continue\n",
    "            \n",
    "        a_vecs, a_counter, a_label = encode_answer(\n",
    "            a_item['answers'],\n",
    "            vocab['answer']\n",
    "        )\n",
    "        source['answer'] = a_vecs\n",
    "        source['answer_counter'] = dict(a_counter)\n",
    "        source['answer_label'] = a_label\n",
    "        \n",
    "        final_sources.append(source)\n",
    "        \n",
    "#         return source\n",
    "    \n",
    "    ''' sample source\n",
    "    {\n",
    "        'question_id': 393226002, \n",
    "        'image_id': 393226, \n",
    "        'question': array([3, 14, 1, 113, 7, 1, 68, 1192, 4877, 4877, 4877,4877, 4877, 4877, 4877, 4877, 4877, 4877, 4877]), \n",
    "        'answer': [489, 489, 489, 489, 489, 489, 489, 489, 489, 489], \n",
    "        'answer_counter': {489: 10}, \n",
    "        'answer_label': 489\n",
    "    }\n",
    "    '''\n",
    "        \n",
    "    # create mindspore dataset\n",
    "    random.shuffle(final_sources)\n",
    "    gen_dataset = ds.GeneratorDataset(\n",
    "        source=Generator(input_list=final_sources), \n",
    "        column_names=[\"data\",\"label\"],\n",
    "        shuffle=False\n",
    "    )\n",
    "    gen_dataset=gen_dataset.batch(batch_size=batch_size,drop_remainder=True)\n",
    "    if dataset == 'train':\n",
    "        gen_dataset = gen_dataset.repeat(config.epoch_size)\n",
    "    \n",
    "    # save to a mindrecord file\n",
    "    if save_mindrecord:\n",
    "        schema_json = {\n",
    "            \"question_id\": {\"type\": \"int32\"}, \n",
    "            \"image_id\": {\"type\": \"int32\"}, \n",
    "            \"question\": {\"type\": \"string\"},\n",
    "            \"answer\": {\"type\": \"string\"},\n",
    "            \"answer_counter\": {\"type\": \"string\"},\n",
    "            \"answer_label\": {\"type\": \"int32\"},\n",
    "        }\n",
    "        for source in final_sources:\n",
    "            source['question'] = base64.b64encode(source['question'])\n",
    "            source['answer'] = base64.b64encode(source['answer'])\n",
    "            source['answer_counter'] = base64.b64encode(source['answer_counter'])\n",
    "            \n",
    "        # index for accelerate data load\n",
    "        indexes = [\"question_id\", \"image_id\", \"answer_label\"]\n",
    "        \n",
    "        if not os.path.exists('mindrecord'):\n",
    "            os.makedirs('mindrecord')\n",
    "        mr_file_path = 'mindrecord/%s.mindrecord' % (dataset)\n",
    "        writer = FileWriter(file_name=mr_file_path, shard_num=4)\n",
    "        writer.add_schema(schema_json, \"data_schema\")\n",
    "        writer.add_index(indexes)\n",
    "        writer.write_raw_data(final_sources)\n",
    "        writer.commit()\n",
    "\n",
    "    \n",
    "    return gen_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b73c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "# do not run!!!\n",
    "# or will crash\n",
    "if not os.path.exists('vocab'):\n",
    "    os.makedirs('vocab')\n",
    "dd = gen_mindspore_dataset()\n",
    "print(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae48c436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
