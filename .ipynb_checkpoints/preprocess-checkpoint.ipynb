{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373c19af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre source download\n",
    "\n",
    "# 36 boxes fixed feature\n",
    "# train and val\n",
    "!wget https://storage.googleapis.com/up-down-attention/trainval_36.zip\n",
    "# test\n",
    "!wget https://storage.googleapis.com/up-down-attention/test2014_36.zip\n",
    "\n",
    "!unzip \"*.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92e7a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "import json\n",
    "import h5py\n",
    "import sys\n",
    "import csv\n",
    "import base64\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac7dee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config \n",
    "\n",
    "config = edict({\n",
    "    \"output_size\": 36,  # max number of object proposals per image\n",
    "    \"output_features\": 2048,  # number of features in each object proposal\n",
    "    \"preprocessed_train_path\": 'image-train.h5',  # path where preprocessed features from the train split are saved to and loaded from\n",
    "    \"preprocessed_val_path\": 'image-val.h5',  # path where preprocessed features from the val split are saved to and loaded from\n",
    "    \"preprocessed_test_path\": 'image-test.h5',  # path where preprocessed features from the test split are saved to and loaded from\n",
    "    \"vocabulary_path\": 'vocab.json',\n",
    "    \"train_num\": 44375,\n",
    "    \"val_num\": 21435,\n",
    "    \"test_num\": 21435,\n",
    "    \"origin_train_num\": 82783,\n",
    "    \"origin_val_num\": 40504,\n",
    "    \"origin_test_num\": 40775,\n",
    "    \"max_answers\": 3129,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6660207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./questions/test.json\n",
      "21435\n",
      "15718 [229385, 393226, 229387, 229391, 458768]\n",
      "./questions/val.json\n",
      "21435\n",
      "15682 [360449, 557059, 229387, 229388, 262161]\n",
      "./questions/train.json\n",
      "44375\n",
      "32077 [131074, 131075, 393223, 393227, 393230]\n"
     ]
    }
   ],
   "source": [
    "# for subset adapt\n",
    "\n",
    "def get_needed_imageid(dataset=\"test\"):\n",
    "    path = './questions/' + dataset + '.json'\n",
    "    print(path)\n",
    "    imageid_set = set()\n",
    "    with open(path, 'r') as f:\n",
    "        t = json.load(f)\n",
    "        print(len(t['questions']))\n",
    "        for question in t['questions']:\n",
    "            imageid_set.add(question['image_id'])\n",
    "    return list(imageid_set)\n",
    "\n",
    "ss = get_needed_imageid('test')\n",
    "print(len(ss), ss[:5])\n",
    "ss = get_needed_imageid('val')\n",
    "print(len(ss), ss[:5])\n",
    "ss = get_needed_imageid('train')\n",
    "print(len(ss), ss[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process image\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "def process_image_feature(dataset=\"test\"):\n",
    "    h5FilePath = config[\"preprocessed_%s_path\" % (dataset)]\n",
    "    print(\"h5 file path:\", h5FilePath)\n",
    "    \n",
    "    num = config[('%s_num' % (dataset))] # number of images in train or in val or in test\n",
    "    print('item num:', num)\n",
    "    \n",
    "    features_shape = (\n",
    "        num,\n",
    "        config.output_features,\n",
    "        config.output_size,\n",
    "    )\n",
    "    boxes_shape = (\n",
    "        num,\n",
    "        4, # top, bottom, left, right\n",
    "        config.output_size,\n",
    "    )\n",
    "    with h5py.File(h5FilePath, 'w', libver='latest') as fd:\n",
    "        features = fd.create_dataset('features', shape=features_shape, dtype='float32')\n",
    "        boxes = fd.create_dataset('boxes', shape=boxes_shape, dtype='float32')\n",
    "        coco_ids = fd.create_dataset('ids', shape=(num,), dtype='int32')\n",
    "        widths = fd.create_dataset('widths', shape=(num,), dtype='int32')\n",
    "        heights = fd.create_dataset('heights', shape=(num,), dtype='int32')\n",
    "        \n",
    "        FIELDNAMES = ['image_id', 'image_w','image_h','num_boxes', 'boxes', 'features']\n",
    "        needed_imageids = get_needed_imageid(dataset)\n",
    "        i = 0\n",
    "        \n",
    "        for Tdataset in ['train', 'eval', 'test']:\n",
    "            tsvFilePath = \"%s2014_resnet101_faster_rcnn_genome_36.tsv\" % (Tdataset)\n",
    "            print(\"tsv file path:\", tsvFilePath)\n",
    "            with open(tsvFilePath, \"r\") as tsvF:\n",
    "                reader = csv.DictReader(tsvF, delimiter='\\t', fieldnames=FIELDNAMES)\n",
    "                origin_num = config[('origin_%s_num' % (Tdataset))]\n",
    "                for _, item in enumerate(tqdm(reader, total=origin_num)):\n",
    "                    cur_id = int(item['image_id'])\n",
    "                    if cur_id not in needed_imageids:\n",
    "                        continue\n",
    "\n",
    "                    coco_ids[i] = int(item['image_id'])\n",
    "                    widths[i] = int(item['image_w'])\n",
    "                    heights[i] = int(item['image_h'])\n",
    "\n",
    "                    buf = base64.decodestring(item['features'].encode('utf8'))\n",
    "                    array = np.frombuffer(buf, dtype='float32')\n",
    "                    array = array.reshape((-1, config.output_features)).transpose() # 36*2048 -> T -> 2048*36\n",
    "                    features[i, :, :array.shape[1]] = array\n",
    "\n",
    "                    buf = base64.decodestring(item['boxes'].encode('utf8'))\n",
    "                    array = np.frombuffer(buf, dtype='float32')\n",
    "                    array = array.reshape((-1, 4)).transpose() # 36*4 -> T -> 4*36\n",
    "                    boxes[i, :, :array.shape[1]] = array\n",
    "\n",
    "                    i = i+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97b445e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./questions/test.json\n",
      "[['what', 'does', 'the', 'truck', 'on', 'the', 'left', 'sell']]\n",
      "./annotations/test.json\n",
      "[['green', 'green', 'green', 'silver', 'green', 'green', 'green', 'green', 'green', 'green']]\n"
     ]
    }
   ],
   "source": [
    "# process vocab\n",
    "\n",
    "_special_chars = re.compile('[^a-z0-9 ]*')\n",
    "_period_strip = re.compile(r'(?!<=\\d)(\\.)(?!\\d)')\n",
    "_comma_strip = re.compile(r'(\\d)(,)(\\d)')\n",
    "_punctuation_chars = re.escape(r';/[]\"{}()=+\\_-><@`,?!')\n",
    "_punctuation = re.compile(r'([{}])'.format(re.escape(_punctuation_chars)))\n",
    "_punctuation_with_a_space = re.compile(r'(?<= )([{0}])|([{0}])(?= )'.format(_punctuation_chars))\n",
    "\n",
    "def prepare_questions(questions_json):\n",
    "    # Tokenize and normalize questions\n",
    "    questions = [q['question'] for q in questions_json['questions']]\n",
    "    for question in questions:\n",
    "        question = question.lower()[:-1]\n",
    "        question = _special_chars.sub('', question)\n",
    "        yield question.split(' ')\n",
    "\n",
    "def prepare_answers(answers_json):\n",
    "    # Normalize answers\n",
    "    answers = [[a['answer'] for a in ans_dict['answers']] for ans_dict in answers_json['annotations']]\n",
    "    def process_punctuation(s):\n",
    "        if _punctuation.search(s) is None:\n",
    "            return s\n",
    "        s = _punctuation_with_a_space.sub('', s)\n",
    "        if re.search(_comma_strip, s) is not None:\n",
    "            s = s.replace(',', '')\n",
    "        s = _punctuation.sub(' ', s)\n",
    "        s = _period_strip.sub('', s)\n",
    "        return s.strip()\n",
    "\n",
    "    for answer_list in answers:\n",
    "        yield list(map(process_punctuation, answer_list))\n",
    "        \n",
    "def extract_vocab(iterable, top_k=None, start=0):\n",
    "    # Turns an iterable of list of tokens into a vocabulary.\n",
    "    all_tokens = itertools.chain.from_iterable(iterable)\n",
    "    counter = Counter(all_tokens)\n",
    "    if top_k:\n",
    "        most_common = counter.most_common(top_k)\n",
    "        most_common = (t for t, c in most_common)\n",
    "    else:\n",
    "        most_common = counter.keys()\n",
    "    # descending in count, then lexicographical order\n",
    "    tokens = sorted(most_common, key=lambda x: (counter[x], x), reverse=True)\n",
    "    vocab = {t: i for i, t in enumerate(tokens, start=start)}\n",
    "    return vocab\n",
    "        \n",
    "def process_questions(dataset=\"test\"):\n",
    "    path = './questions/' + dataset + '.json'\n",
    "    print(path)\n",
    "    with open(path, 'r') as f:\n",
    "        t = json.load(f)\n",
    "        questions = list(prepare_questions(t))\n",
    "        print(questions[:1])\n",
    "        question_vocab = extract_vocab(questions, start=1)\n",
    "        return question_vocab\n",
    "    return None\n",
    "\n",
    "def process_answers(dataset=\"test\"):\n",
    "    path = './annotations/' + dataset + '.json'\n",
    "    print(path)\n",
    "    with open(path, 'r') as f:\n",
    "        t = json.load(f)\n",
    "        answers = list(prepare_answers(t))\n",
    "        print(answers[:1])\n",
    "        answer_vocab = extract_vocab(answers, top_k=config.max_answers)\n",
    "        return answer_vocab\n",
    "    return None\n",
    "        \n",
    "        \n",
    "def process_vocab(dataset=\"test\"):\n",
    "    question_vocab = process_questions(dataset)\n",
    "    answer_vocab = process_answers(dataset)\n",
    "    vocabs = {\n",
    "        'question': question_vocab,\n",
    "        'answer': answer_vocab,\n",
    "    }\n",
    "    with open(config.vocabulary_path, 'w') as fd:\n",
    "        json.dump(vocabs, fd)\n",
    "\n",
    "    \n",
    "process_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8105b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
