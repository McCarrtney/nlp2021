{"metadata": {"language_info": {"name": "python", "version": "3.7.6", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kernelspec": {"name": "mindspore-python3.7-aarch64", "display_name": "Mindspore-python3.7-aarch64", "language": "python"}, "interpreter": {"hash": "dd511a26238e6ae0a0128ef6310b8218f6de319ae2077d3e90e8e15d9f9e182d"}}, "nbformat_minor": 5, "nbformat": 4, "cells": [{"cell_type": "code", "source": "import mindspore\nfrom mindspore import Tensor\nimport mindspore.ops as ops\nimport mindspore.nn as nn\nfrom mindspore.train.model import Model\nfrom mindspore.nn.metrics import Accuracy\nfrom mindspore.train.callback import LossMonitor, TimeMonitor\nimport mindspore.dataset as ds\nfrom mindspore import context\n\nimport numpy as np\nimport h5py\nimport json\nimport re\nimport moxing as mox\nimport pickle", "metadata": {"trusted": true}, "execution_count": 83, "outputs": []}, {"cell_type": "code", "source": "context.set_context(mode=context.GRAPH_MODE, device_target='Ascend', device_id=0)", "metadata": {"trusted": true}, "execution_count": 84, "outputs": []}, {"cell_type": "code", "source": "mox.file.copy_parallel(src_url=\"s3://nlp-xyz/word-vec/proj/nlp2021/annotations\", dst_url='./annotations')\nmox.file.copy_parallel(src_url=\"s3://nlp-xyz/word-vec/proj/nlp2021/questions\", dst_url='./questions')\nmox.file.copy_parallel(src_url=\"s3://nlp-xyz/word-vec/proj/nlp2021/vocab\", dst_url='./vocab')\nmox.file.copy_parallel(src_url=\"s3://nlp-xyz/word-vec/proj/image_features\", dst_url='./image_features')\nmox.file.copy_parallel(src_url=\"s3://nlp-xyz/word-vec/proj/dataset.pkl\", dst_url='./dataset.pkl')", "metadata": {"trusted": true}, "execution_count": 85, "outputs": []}, {"cell_type": "code", "source": "class text_encoder(nn.Cell):\n    def __init__(self, vocab_size, embedding_size=2048, max_len=100):\n        super(text_encoder, self).__init__()\n\n        self.vocab_size = vocab_size\n        self.embedding_size = embedding_size\n        self.max_len = max_len\n\n        self.embed = nn.Embedding(vocab_size, embedding_size)\n        self.WQ = nn.Dense(embedding_size, embedding_size)\n        self.WK = nn.Dense(embedding_size, embedding_size)\n        self.WV = nn.Dense(embedding_size, embedding_size)\n\n        self.pe = self.pe_gen()\n        \n        self.matmul = ops.BatchMatMul()\n        self.transpose = ops.Transpose()\n        self.softmax = nn.Softmax()\n        self.reducemax = ops.ReduceMax(keep_dims=True)\n        self.print = ops.Print()\n\n    def pe_gen(self):\n        pe = np.empty((self.max_len, self.embedding_size))\n        even = 10000**(np.arange(self.embedding_size, step=2)/self.embedding_size)\n        odd = 10000**(np.arange(self.embedding_size-1, step=2)/self.embedding_size)\n        for pos in range(self.max_len):\n            pe[pos, ::2] = np.sin(pos/even)\n            pe[pos, 1::2] = np.cos(pos/odd)\n        return Tensor(pe.astype('float32'))\n\n    def construct(self, x, squeeze=False): # len\n        x = self.embed(x) # batch * len * emb\n        x = x + self.pe[:x.shape[1]] # batch * len * emb\n        \n        Q = self.WQ(x) # batch * len * emb\n        K = self.WK(x) # batch * len * emb\n        V = self.WV(x) # batch * len * emb\n        \n        QK = self.matmul(Q, self.transpose(K, (0,2,1))) # batch * len * len\n        Z = self.matmul(self.softmax(QK), V) # batch * len * emb\n        \n        if squeeze:\n            Z = self.reducemax(Z, 1) # batch * 1 * emb\n        return Z # batch * 1 * emb", "metadata": {"trusted": true}, "execution_count": 86, "outputs": []}, {"cell_type": "code", "source": "class block(nn.Cell):\n    def __init__(self, hidden_size=2048):\n        super(block, self).__init__()\n        self.Wq = nn.Dense(hidden_size, hidden_size)\n        self.Wi = nn.Dense(hidden_size, hidden_size, has_bias=False)\n        self.Wp = nn.Dense(hidden_size, 1)\n        self.transpose = ops.Transpose()\n        self.tanh = nn.Tanh()\n        self.softmax = nn.Softmax(0)\n        self.reduce_sum = ops.ReduceSum()\n        self.matmul = ops.BatchMatMul()\n    \n    def construct(self, v_i, v_q):\n        encoded_q = self.Wq(v_q) # batch * 1 * hid\n        encoded_i = self.Wi(self.transpose(v_i, (0,2,1))) # batch * 36 * hid\n        hA = self.tanh(encoded_q + encoded_i) # batch * 36 * hid\n        pI = self.softmax(self.Wp(hA)) # batch * 36 * 1\n        vI = self.matmul(self.transpose(pI, (0,2,1)), hA) # batch * 1 * hid\n        u = vI + v_q\n        return u\n\nclass SAN(nn.Cell):\n    def __init__(self, hidden_size=2048):\n        super(SAN, self).__init__()\n        \n        self.block0 = block(hidden_size)\n        self.block1 = block(hidden_size)\n        self.block2 = block(hidden_size)\n        self.block3 = block(hidden_size)\n\n    def construct(self, v_i, v_q): # v_i: batch * 36 * hid, v_q: batch * 1 * hid\n        u0 = self.block0(v_i, v_q)\n        u1 = self.block1(v_i, u0)\n        u2 = self.block2(v_i, u1)\n        u3 = self.block3(v_i, u2)\n\n        return u3 # 1 * hid", "metadata": {"trusted": true}, "execution_count": 87, "outputs": []}, {"cell_type": "code", "source": "class VQA(nn.Cell):\n    def __init__(self, question_vocab_size, answer_vocab_size, hidden_size=2048):\n        super(VQA, self).__init__()\n        self.question_encoder = text_encoder(question_vocab_size, hidden_size)\n        self.answer_encoder = text_encoder(answer_vocab_size, hidden_size)\n        self.SAN = SAN(hidden_size)\n        self.matmul = ops.BatchMatMul()\n        self.transpose = ops.Transpose()\n        self.argmax = ops.Argmax()\n        self.softmax = nn.Softmax()\n        self.cast = ops.Cast()\n        self.print = ops.Print()\n\n    def construct(self, x):\n        v_i = x[:, :-1]\n        q = self.cast(x[:, -2:-1, :20], mindspore.int32).view(x.shape[0],-1) # \n        ans = self.cast(x[:, -2:-1, 20:], mindspore.int32).view(x.shape[0],-1)\n        v_q = self.question_encoder(q, True) # batch * 1 * hid\n        v_ans = self.answer_encoder(ans) # batch * n * hid\n        uk = self.SAN(v_i, v_q) # batch * 1 * hid\n        score = self.matmul(uk, self.transpose(v_ans, (0,2,1)))\n        prob = self.softmax(score) # batch * 1 * n\n        chosen = self.argmax(score)\n        \n        # return score.view(x.shape[0], -1)\n        return score\n        # return prob\n        # return chosen", "metadata": {"trusted": true}, "execution_count": 88, "outputs": []}, {"cell_type": "code", "source": "# vqa = VQA(20000, 20000)", "metadata": {"trusted": true}, "execution_count": 89, "outputs": []}, {"cell_type": "code", "source": "# im = np.random.randn(64, 2048, 36)\n# text = np.concatenate([np.arange(36)]*64).reshape(64,1,36)\n# syn = Tensor(np.concatenate((im, text), 1).astype('float32'))", "metadata": {"trusted": true}, "execution_count": 90, "outputs": []}, {"cell_type": "code", "source": "# vqa(syn)", "metadata": {"trusted": true}, "execution_count": 91, "outputs": []}, {"cell_type": "code", "source": "categories = ['train', 'val', 'test']", "metadata": {"trusted": true}, "execution_count": 92, "outputs": []}, {"cell_type": "code", "source": "questions = {}\nannotations = {}\nfeatures = {}\n\nfor c in categories:\n    quest_json = open('./questions/{}.json'.format(c))\n    quest = json.load(quest_json)\n    quest_json.close()\n    questions[c] = quest['questions']\n\n    anno_json = open('./annotations/{}.json'.format(c))\n    anno = json.load(anno_json)\n    anno_json.close()\n    annotations[c] = anno['annotations']\n\n    try:\n        features[c] = h5py.File('./image_features/image-{}.h5'.format(c), 'r')\n    except:\n        pass\n\nvocab_file = open('./vocab/test.json')\nvocab = json.load(vocab_file)\nvocab_file.close()", "metadata": {"trusted": true}, "execution_count": 93, "outputs": []}, {"cell_type": "code", "source": "_special_chars = re.compile('[^a-z0-9 ]*')\n_period_strip = re.compile(r'(?!<=\\d)(\\.)(?!\\d)')\n_comma_strip = re.compile(r'(\\d)(,)(\\d)')\n_punctuation_chars = re.escape(r';/[]\"{}()=+\\_-><@`,?!')\n_punctuation = re.compile(r'([{}])'.format(re.escape(_punctuation_chars)))\n_punctuation_with_a_space = re.compile(r'(?<= )([{0}])|([{0}])(?= )'.format(_punctuation_chars))\n\ndef question_tokenize(question):\n    question = question.lower()[:-1]\n    question = _special_chars.sub('', question)\n    return question.split(' ')\n\ndef answer_tokenize(answer):\n    if _punctuation.search(answer) is None:\n        return answer\n    answer = _punctuation_with_a_space.sub('', answer)\n    if re.search(_comma_strip, answer) is not None:\n        answer = answer.replace(',', '')\n    answer = _punctuation.sub(' ', answer)\n    answer = _period_strip.sub('', answer)\n    return answer.strip()\n\ndef question2vec(question):\n    return Tensor([vocab['question'][word] for word in question_tokenize(question)])\n\ndef answer2vec(answer):\n    return Tensor([vocab['answer'][answer_tokenize(answer)]])", "metadata": {"trusted": true}, "execution_count": 94, "outputs": []}, {"cell_type": "code", "source": "data_file = open('dataset.pkl', 'rb')\ndataset = pickle.load(data_file)\ndata_file.close()", "metadata": {"trusted": true}, "execution_count": 95, "outputs": []}, {"cell_type": "code", "source": "nb_answers = max([max([data['answer_label'] for data in dataset[c]]) for c in categories]) + 1", "metadata": {"trusted": true}, "execution_count": 96, "outputs": []}, {"cell_type": "code", "source": "nb_choices = 16", "metadata": {"trusted": true}, "execution_count": 97, "outputs": []}, {"cell_type": "code", "source": "vqa = VQA(len(vocab['question']), len(vocab['answer']))", "metadata": {"trusted": true}, "execution_count": 98, "outputs": []}, {"cell_type": "code", "source": "# im = np.random.randn(64, 2048, 36)\n# text = np.concatenate([np.arange(36)]*64).reshape(64,1,36)\n# syn = Tensor(np.concatenate((im, text), 1).astype('float32'))", "metadata": {"trusted": true}, "execution_count": 99, "outputs": []}, {"cell_type": "code", "source": "# a = vqa(syn)", "metadata": {"trusted": true}, "execution_count": 100, "outputs": []}, {"cell_type": "code", "source": "id2idx = {'train' : {}, 'test' : {}, 'val' : {}}\nfor c in categories:\n    id2idx[c] = {id : i for i, id in enumerate(features[c]['ids'])}", "metadata": {"trusted": true}, "execution_count": 101, "outputs": []}, {"cell_type": "code", "source": "id2idx_file = open('id2idx.pkl', 'wb')\npickle.dump(id2idx, id2idx_file)\nid2idx_file.close()\nmox.file.copy_parallel(src_url=\"./id2idx.pkl\", dst_url='s3://nlp-xyz/word-vec/proj/id2idx.pkl')", "metadata": {"trusted": true}, "execution_count": 102, "outputs": []}, {"cell_type": "code", "source": "train_data_set = []\nfor i, data in enumerate(dataset['train']):\n    if i >= 64:\n        break\n    answers = np.empty(nb_choices).astype('int32')\n    answers[0] = data['answer_label']\n    answers[1:] = np.random.choice(nb_answers, nb_choices - 1, replace=False)\n    while answers[0] in answers[1:]:\n        answers[1:] = np.random.choice(nb_answers, nb_choices - 1, replace=False)\n    np.random.shuffle(answers)\n    label = np.where(answers==data['answer_label'])[0].astype('int32')\n    question_answer = np.concatenate((data['question'].astype('int32').reshape(1,-1), answers.reshape(1,-1)),1)\n    image_text = np.concatenate((features['train']['features'][id2idx['train'][data['image_id']]].astype('float32'), \n                                question_answer.astype('float32')), 0)\n    train_data_set.append(\n        #(features['train']['features'][id2idx['train'][data['image_id']]].astype('float32'), \n         #data['question'].astype('int32'),\n         #answers, \n         #label)\n        \n        (image_text, label)\n    )", "metadata": {"trusted": true}, "execution_count": 103, "outputs": []}, {"cell_type": "code", "source": "class Generator():\n    def __init__(self, input_list):\n        self.input_list=input_list\n    def __getitem__(self,item):\n        return (self.input_list[item][0], self.input_list[item][1])\n        # return (self.input_list[item][0], self.input_list[item][1], self.input_list[item][2], self.input_list[item][3])\n        # return (self.input_list[item]['data'], self.input_list[item]['label'])\n    def __len__(self):\n        return len(self.input_list)", "metadata": {"trusted": true}, "execution_count": 104, "outputs": []}, {"cell_type": "code", "source": "trainset = ds.GeneratorDataset(\n    source=Generator(input_list=train_data_set), \n    # column_names=[\"v_i\", 'q', 'ans',\"label\"],\n    # column_names=[\"v_i\", \"label\"],\n    column_names=[\"data\", \"label\"],\n    shuffle=False\n)\ntrainset = trainset.batch(batch_size=64)\n# trainset = trainset.repeat(2)", "metadata": {"trusted": true}, "execution_count": 105, "outputs": []}, {"cell_type": "code", "source": "# next(trainset.create_dict_iterator())", "metadata": {"trusted": true}, "execution_count": 110, "outputs": []}, {"cell_type": "code", "source": "opt = nn.Adam(vqa.get_parameters(), learning_rate=0.001)\nloss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)\nmodel = Model(vqa, loss_fn=loss, optimizer=opt, metrics={'acc': Accuracy()})", "metadata": {"trusted": true}, "execution_count": 107, "outputs": []}, {"cell_type": "code", "source": "time_cb = TimeMonitor(data_size=64)\nloss_cb = LossMonitor()", "metadata": {"trusted": true}, "execution_count": 108, "outputs": []}, {"cell_type": "code", "source": "model.train(1, trainset, callbacks=[time_cb, loss_cb])\nprint('train_success')", "metadata": {"trusted": true}, "execution_count": 111, "outputs": []}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": []}]}